# OpenAI-совместимый консольный клиент

Этот проект представляет собой консольного клиента для взаимодействия с OpenAI-совместимыми моделями. Программа позволяет пользователю вводить текстовые запросы и получать ответы от модели в режиме реального времени.

## Основные возможности

- Поддержка диалога с OpenAI-совместимыми моделями.
- Возможность ввода сообщений пользователем.
- Вывод ответов модели.
- Поддержка выхода из программы с помощью команды `quit`.
- Возможность изменения температуры с помощью команды `temp x.x`
- Отображение времени выполнения запроса и расхода токенов
- **Новое:** Интеграция с RAG (Retrieval Augmented Generation) для работы с документами
- **Новое:** Поддержка обработки PDF файлов и поиска по ним
- **Новое:** Интеграция с векторной базой данных Qdrant
- **Новое:** Поддержка GPU-оптимизации для обработки эмбеддингов
- **Новое:** Асинхронная обработка для улучшения производительности

## Требования

- Python 3.7 или выше
- Установленные пакеты:
  - `openai`
  - `python-dotenv`
  - `mcp`
  - `httpx`
  - `docker`
  - `PyPDF2`
  - `qdrant-client`
  - `numpy`
  - `requests`
  - `aiohttp`

## Установка

1. Клонируйте репозиторий:
   ```bash
   git clone https://github.com/yourusername/openai-console-client.git
   cd openai-console-client
   ```

2. Установите зависимости:
   ```bash
   pip install -r requirements.txt
   ```

3. Создайте файл `.env` в корне проекта и добавьте API-ключ:
   ```env
   OPENAI_API_KEY=your_openai_api_key_here
   # или используйте существующий ZAI_API_KEY
   ZAI_API_KEY=your_zai_api_key_here
   # или настройте OpenAI-совместимый сервис
   OPENAI_BASE_URL=https://your-openai-compatible-service.com/v1
   MODEL_NAME=gpt-3.5-turbo  # или другая модель
   OPEN_WEATHER=api_key
   ```

## Использование

1. Запустите программу:
   ```bash
   python main.py
   ```

2. Введите свои сообщения. Для выхода введите `quit`.
3. Для изменения температуры используйте команду `temp 0.7` (где 0.7 - значение температуры)

## Пример использования

```
==================================================
ROBOT Чат-клиент с MCP инструментами
Команды: quit/exit, save <имя>, load <имя>, temp <0-2>, clear, print
RAG команды: /rag (вкл/выкл), /rag <вопрос> (вопрос с RAG), /rag rerank (вкл/выкл реранкер), /rag threshold <значение> (установить порог)
==================================================

USER Вы: Привет, как дела?

THERMOMETER Температура установлена: 0.7
ASSISTANT Ассистент: Привет! У меня всё отлично, спасибо. Как у тебя?

TIME Время: 1.25с | Инструментов: 2 (STDIO: 1, Docker: 1)

USER Вы: temp 0.5
THERMOMETER Температура установлена: 0.5

USER Вы: Как дела?
THERMOMETER Температура установлена: 0.5
ASSISTANT Ассистент: У меня всё стабильно, спасибо за интерес. Как могу помочь вам сегодня?

TIME Время: 0.87с | Инструментов: 2 (STDIO: 1, Docker: 1)

USER Вы: quit
GOODBYE До свидания!
```

## Дополнительные компоненты

### MCP Сервер и Клиент

Проект теперь включает полную интеграцию MCP (Model Context Protocol):
- MCP сервер с инструментами погоды и другими утилитами
- MCP клиент, интегрированный в основной чат
- Поддержка Docker MCP сервера для распределенных инструментов

#### Использование MCP интеграции

1. Запустите главный чат-клиент:
   ```bash
   python main.py
   ```

2. При запуске клиент автоматически подключается к MCP серверу и показывает доступные инструменты:
   - `stub-tool`: Простой инструмент для демонстрации, возвращает фиктивный ответ
   - `get-weather`: Получает текущую погоду для указанного города с использованием OpenWeatherMap API
   - `get_weather_forecast`: Получает прогноз погоды на несколько дней
   - `convert_temperature`: Конвертирует температуру между различными единицами измерения
   - `save_weather_data`: Сохраняет погодные данные в JSON файл
   - `execute_python_code`: Выполняет Python код в Docker контейнере

3. Для вызова инструментов используйте формат в чате:
   - `get-weather Париж` - получить погоду для Парижа
   - `get-weather London` - получить погоду для Лондона
   - `stub-tool test` - вызвать заглушку инструмента с тестовым входом
   - `execute_python_code print('Hello from Docker')` - выполнить Python код в Docker

Для работы инструментов `get-weather` и `get_weather_forecast` необходим API-ключ OpenWeatherMap, указанный в переменной окружения `OPEN_WEATHER` в файле `.env`.

#### Использование MCP сервера отдельно

Если вы хотите запустить MCP сервер отдельно:
   ```bash
   python mcp_server.py
   ```

Сервер реализует протокол Model Context Protocol и может взаимодействовать с совместимыми MCP клиентами.

### RAG (Retrieval Augmented Generation) и PDF обработка

Проект включает мощную систему RAG для работы с документами:

#### Основные возможности RAG:
- Поиск по векторной базе данных Qdrant
- Генерация эмбеддингов с помощью Ollama
- Поддержка реранкинга результатов
- Настройка порога релевантности
- Интеграция с чатом для ответов на основе документов

#### Команды RAG:
- `/rag` - включить/выключить режим RAG
- `/rag <вопрос>` - задать вопрос с использованием RAG
- `/rag rerank` - включить/выключить реранкер
- `/rag threshold 0.5` - установить порог релевантности (0-1)

#### Запуск пайплайна обработки PDF:
Для подготовки документов к поиску:

```bash
# Синхронная версия
python pdf_pipeline.py --pdf_path path/to/your/file.pdf

# Асинхронная версия (рекомендуется для GPU)
python pdf_pipeline_async.py --pdf_path path/to/your/file.pdf --max_concurrent 10

# Оптимизированная версия для GPU
python run_pipeline_gpu.py --pdf_path path/to/your/file.pdf --use_async --max_concurrent 15
```

#### Параметры пайплайна:
- `--pdf_path` (обязательный): Путь к обрабатываемому PDF файлу
- `--chunk_size`: Размер чанка текста (по умолчанию 1024/2048 для GPU)
- `--overlap`: Перекрытие между чанками (по умолчанию 50)
- `--collection_name`: Название коллекции в Qdrant (по умолчанию pdf_chunks)
- `--embedding_model`: Используемая модель Ollama (по умолчанию qwen3-embedding:latest)
- `--ollama_host`: Адрес API Ollama (по умолчанию http://localhost:11434)
- `--max_concurrent` (для асинхронной версии): Количество одновременных запросов (по умолчанию 10, рекомендуется 15 для RTX 4070)

### Настройка GPU для максимальной производительности

Для максимальной производительности при генерации эмбеддингов с помощью Ollama и вашей видеокарты NVIDIA RTX 4070:

1. Установите CUDA Toolkit с официального сайта NVIDIA
2. Убедитесь, что версия драйвера вашей видеокарты поддерживает устанавливаемую версию CUDA
3. Запустите Ollama с настройками, оптимизированными под вашу видеокарту:

```bash
# Установите переменные окружения для использования GPU
set OLLAMA_NUM_PARALLEL=10
set OLLAMA_MAX_LOADED_MODELS=2
set OLLAMA_NOHISTORY=1

# Если вы хотите ограничить использование памяти (например, для 12GB VRAM)
# set OLLAMA_GPU_MEMORY=10240

# Запустите Ollama
ollama serve
```

4. Используйте асинхронную версию пайплайна для лучшего использования GPU:
```bash
python pdf_pipeline_async.py --pdf_path your_file.pdf --max_concurrent 10
```

### Дополнительные команды чата

- `quit` или `exit` - выход из программы
- `clear` - очистка истории разговора
- `print` - вывод истории разговора
- `save <имя>` - сохранение разговора в файл
- `load <имя>` - загрузка разговора из файла
- `temp <0-2>` - изменение температуры модели
- `/rag` - включение/выключение режима RAG
- `/rag rerank` - включение/выключение реранкера
- `/rag threshold <значение>` - установка порога релевантности